{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "from cntk.device import try_set_default_device, gpu\n",
    "try_set_default_device(gpu(0))\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix the random seed so that LR examples are repeatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory is ../training_data/input/k-fold/1\n",
      "Train-data path is ../training_data/input/k-fold/1/train.txt\n",
      "Test-data path is ../training_data/input/k-fold/1/test.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import cntk\n",
    "global f \n",
    "global index_model\n",
    "index_model = 1\n",
    "\n",
    "f = open(\"log2.txt\", 'a')\n",
    "\n",
    "# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\n",
    "def create_reader(path, is_training, input_dim, num_label_classes):   \n",
    "    labelStream = cntk.io.StreamDef(field='label', shape=num_label_classes, is_sparse=False)\n",
    "    featureStream = cntk.io.StreamDef(field='features', shape=input_dim, is_sparse=False)\n",
    "    \n",
    "    deserailizer = cntk.io.CTFDeserializer(path, cntk.io.StreamDefs(labels = labelStream, features = featureStream))\n",
    "            \n",
    "    return cntk.io.MinibatchSource(deserailizer,\n",
    "       randomize = is_training, max_sweeps = cntk.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "# Ensure the training and test data is generated and available for this tutorial.\n",
    "# We search in two locations in the toolkit for the cached MNIST data set.\n",
    "data_found = False\n",
    "\n",
    "for data_dir in [\"../training_data/input/k-fold/1\"]:\n",
    "    train_file = os.path.join(data_dir, \"train.txt\")\n",
    "    test_file = os.path.join(data_dir, \"test.txt\")\n",
    "    if os.path.isfile(train_file) and os.path.isfile(test_file):\n",
    "        data_found = True\n",
    "        break\n",
    "        \n",
    "if not data_found:\n",
    "    raise ValueError(\"Your data files are not available. Please check it out if you put them in the same fol\")\n",
    "    \n",
    "print(\"Data directory is {0}\".format(data_dir))\n",
    "print(\"Train-data path is \" + train_file)\n",
    "print(\"Test-data path is \" + test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_criterion_function(model, labels):\n",
    "    loss = cntk.cross_entropy_with_softmax(model, labels)\n",
    "    errs = cntk.classification_error(model, labels)\n",
    "    return loss, errs # (model, labels) -> (loss, error metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a utility function to compute the moving average sum.\n",
    "# A more efficient implementation is possible with np.cumsum() function\n",
    "def moving_average(a, w=5):\n",
    "    if len(a) < w:\n",
    "        return a[:]    # Need to send a copy of the array\n",
    "    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n",
    "\n",
    "\n",
    "# Defines a utility that prints the training progress\n",
    "def print_training_progress(trainer, mb, frequency, verbose=1):\n",
    "    training_loss = \"NA\"\n",
    "    eval_error = \"NA\"\n",
    "\n",
    "    if mb%frequency == 0:\n",
    "        training_loss = trainer.previous_minibatch_loss_average\n",
    "        eval_error = trainer.previous_minibatch_evaluation_average\n",
    "        if verbose: \n",
    "            print (\"\\t\\tMinibatch: {0}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n",
    "        \n",
    "    return mb, training_loss, eval_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_adaptive_learner(model, base_lr_per_sample, minibatch_size, num_samples_per_sweep):\n",
    "        lr_schedule = get_adaptive_learning_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep)\n",
    "        #momentum_time_constant = -minibatch_size / np.log(0.98)\n",
    "        #momentum_time_constant = [3000]\n",
    "        l2_reg_weight = 0.0001  # 0.0001\n",
    "        #mm_schedule = cntk.momentum_as_time_constant_schedule(momentum_time_constant)\n",
    "        mm_schedule = cntk.momentum_schedule(0.90)\n",
    "\n",
    "        learner = cntk.momentum_sgd(model.parameters, lr_schedule, mm_schedule,\n",
    "                               l2_regularization_weight=l2_reg_weight, unit_gain=True)\n",
    "\n",
    "        return learner\n",
    "    \n",
    "def get_adaptive_learning_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep):\n",
    "        return cntk.learners.learning_parameter_schedule(base_lr_per_sample, minibatch_size, num_samples_per_sweep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_test(train_reader, test_reader, model_func ,index_model, file_writer, num_sweeps_to_train_with=10):\n",
    "    \n",
    "    # Instantiate the model function; x is the input (feature) variable \n",
    "    # We will scale the input image pixels within 0-1 range by dividing all input value by 255.\n",
    "    model = model_func(input/255)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, label)\n",
    "    \n",
    "    minibatch_size = 64\n",
    "    num_samples_per_sweep = len( open(train_file,'r').readlines() )\n",
    "    num_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size\n",
    "    \n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    k = int(num_samples_per_sweep / 64.0)\n",
    "    learning_rate = ([0.2]*(k*2))+([0.1]*(k*2))+([0.05]*(k*2))+([0.025]*(k*2))+([0.175]*(k*2))\n",
    "    \n",
    "    lr_schedule = cntk.learning_rate_schedule(learning_rate, cntk.UnitType.minibatch)\n",
    "    learner =  set_adaptive_learner(z, learning_rate, minibatch_size, num_samples_per_sweep)\n",
    "#     learner = cntk.sgd(z.parameters, lr_schedule)\n",
    "    trainer = cntk.Trainer(z, (loss, label_error), [learner])\n",
    "    \n",
    "    # Initialize the parameters for the trainer\n",
    "    \n",
    "    \n",
    "    # Map the data streams to the input and labels.\n",
    "    input_map={\n",
    "        label  : train_reader.streams.labels,\n",
    "        input  : train_reader.streams.features\n",
    "    } \n",
    "    \n",
    "    # Uncomment below for more detailed logging\n",
    "    training_progress_output_freq = 500\n",
    "     \n",
    "    # Start a timer\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(0, int(num_minibatches_to_train)):\n",
    "        # Read a mini batch from the training data file\n",
    "        data=train_reader.next_minibatch(minibatch_size, input_map=input_map) \n",
    "        trainer.train_minibatch(data)\n",
    "        print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n",
    "     \n",
    "    # Print training time\n",
    "    string_training_time = \"\\t\\tTraining took {:.1f} sec\".format(time.time() - start)\n",
    "    string_append_and_print(string_training_time, file_writer)\n",
    "    \n",
    "    # Test the model\n",
    "    test_input_map = {\n",
    "        label  : test_reader.streams.labels,\n",
    "        input  : test_reader.streams.features\n",
    "    }\n",
    "\n",
    "    # Test data for trained model\n",
    "    test_minibatch_size = 64\n",
    "    num_samples = len( open(test_file,'r').readlines() )\n",
    "    num_minibatches_to_test = num_samples // test_minibatch_size\n",
    "    \n",
    "    test_result = 0.0   \n",
    "\n",
    "    for i in range(num_minibatches_to_test):\n",
    "    \n",
    "        # We are loading test data in batches specified by test_minibatch_size\n",
    "        # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n",
    "        # with one pixel per dimension that we will encode / decode with the \n",
    "        # trained model.\n",
    "        data = test_reader.next_minibatch(test_minibatch_size, input_map=test_input_map)\n",
    "        eval_error = trainer.test_minibatch(data)\n",
    "        test_result = test_result + eval_error\n",
    "    \n",
    "    # Average of evaluation errors of all test minibatches\n",
    "    string_average_test_error = \"\\t\\tAverage test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test)\n",
    "    string_append_and_print(string_average_test_error, file_writer)\n",
    "    \n",
    "    #save model whem error late less number target\n",
    "    path_model = \"../testing_data/model/model{0:.2f}.model\".format(test_result*100 / num_minibatches_to_test)\n",
    "#     if((test_result*100 / num_minibatches_to_test) < 9) :\n",
    "    string_saved_model = \"\\t\\tmodel saved from data set {0:.2f}\".format(test_result*100 / num_minibatches_to_test)\n",
    "    string_append_and_print(string_saved_model, file_writer)\n",
    "\n",
    "    z.save(path_model)\n",
    "    return (test_result*100 / num_minibatches_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_train_test(train_file,test_file,input_dim, index_model, file_writer ,num_output_classes, filter, stride, filters):\n",
    "    global z\n",
    "    z = create_model(input,num_output_classes, filter, stride, filters)\n",
    "    reader_train = create_reader(train_file, True, input_dim, num_output_classes)\n",
    "    reader_test = create_reader(test_file, False, input_dim, num_output_classes)\n",
    "    return train_test(reader_train, reader_test, z, index_model, file_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input, out_dims, filter, stride, filters):\n",
    "    with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n",
    "        h = C.layers.Convolution2D(filter_shape=(filter,filter), \n",
    "                                       num_filters=filters[0], \n",
    "                                       strides=(stride,stride), \n",
    "                                       pad=True)(input)\n",
    "        h = C.layers.Convolution2D(filter_shape=(filter,filter), \n",
    "                                       num_filters=filters[1], \n",
    "                                       strides=(stride,stride), \n",
    "                                       pad=True)(h)\n",
    "        h = C.layers.Convolution2D(filter_shape=(filter,filter), \n",
    "                                       num_filters=filters[2], \n",
    "                                       strides=(stride,stride), \n",
    "                                       pad=True)(h)\n",
    "        h = C.layers.Dense(9,activation=None)(h)\n",
    "#         h = C.layers.Dense(6,activation=None)(h)\n",
    "        r = C.layers.Dense(out_dims, activation=None)(h)\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim_model = ( 3 , 31, 31)\n",
    "input_dim = 3 * 31 * 31\n",
    "num_output_classes = 3\n",
    "input = cntk.input_variable(input_dim_model)  # สังเกตว่าเราใช้ input_dim_model เป็นพารามิเตอร์แทนการใช้ input_dim\n",
    "label = cntk.input_variable(num_output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_append_and_print(line, file_writer) :\n",
    "    file_writer.write(line + \"\\n\")\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process1(filter, stride, filters, path_log):\n",
    "    error_rates = []\n",
    "    sum_error_rate = 0\n",
    "    file_writer = open(path_log, 'a')\n",
    "    \n",
    "    string_filter_details = \"filter => {:d}, stride => {:d}, dept => {:d}, {:d} {:d} {:d}\".format(filter, stride, len(filters), filters[0], filters[1], filters[2])\n",
    "    string_append_and_print(string_filter_details, f)\n",
    "\n",
    "    file_writer.close()\n",
    "    file_writer = open(path_log, 'a')\n",
    "    \n",
    "    #k-flod validation = 10\n",
    "    for i in range(1,11):\n",
    "        #address file train and test\n",
    "        data_dir = '../training_data/input/k-fold/' + str(i)\n",
    "        string_index_data_set = '\\tData Set : ' + str(i)\n",
    "\n",
    "        string_append_and_print(string_index_data_set, file_writer)\n",
    "        \n",
    "        #read file train and test\n",
    "        train_file = os.path.join(data_dir, \"train.txt\")\n",
    "        test_file = os.path.join(data_dir, \"test.txt\")\n",
    "        \n",
    "        #start train and test\n",
    "        error_rate = do_train_test(train_file, test_file, input_dim, i, file_writer, num_output_classes, filter, stride, filters)\n",
    "        sum_error_rate += error_rate\n",
    "        \n",
    "        #append error rate to list for calculate average error late\n",
    "        error_rates.append(error_rate)\n",
    "\n",
    "        if index_model > 1 :\n",
    "            break\n",
    "\n",
    "    string_file_end_line = \"\\tAverage model error: {0:.2f}%\\n\\tmin model error: {0:.2f}%\\n\\t>>>>>>>>>>>>>>>>>>\\n>>>>>>>>>>>>>>>>>>\".format(sum_error_rate/10,min(error_rates))     \n",
    "    string_append_and_print(string_file_end_line, file_writer)\n",
    "    \n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter => 5, stride => 2, dept => 3, 128 512 1024\n",
      "\tData Set : 1\n",
      "\t\tMinibatch: 0, Loss: 1.0982, Error: 51.56%\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<built-in function MinibatchSource_get_next_minibatch> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/home/anaconda3/envs/cntk36/lib/python3.6/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamInformation___hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_StreamInformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in method 'StreamInformation___hash__', argument 1 of type 'CNTK::StreamInformation *'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1661d3ca7e9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"log.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprocess1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_log\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-35bae5090dd9>\u001b[0m in \u001b[0;36mprocess1\u001b[0;34m(filter, stride, filters, path_log)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#start train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0merror_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_train_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0msum_error_rate\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-e7f915800851>\u001b[0m in \u001b[0;36mdo_train_test\u001b[0;34m(train_file, test_file, input_dim, index_model, file_writer, num_output_classes, filter, stride, filters)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreader_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-aead9b10da86>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(train_reader, test_reader, model_func, index_model, file_writer, num_sweeps_to_train_with)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_minibatches_to_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Read a mini batch from the training data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint_training_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_progress_output_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/cntk36/lib/python3.6/site-packages/cntk/internal/swig_helper.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mmap_if_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/cntk36/lib/python3.6/site-packages/cntk/io/__init__.py\u001b[0m in \u001b[0;36mnext_minibatch\u001b[0;34m(self, minibatch_size_in_samples, input_map, device, num_data_partitions, partition_index)\u001b[0m\n\u001b[1;32m    327\u001b[0m                                             \u001b[0mminibatch_size_in_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                                             \u001b[0mnum_data_partitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                                             partition_index, device)\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/envs/cntk36/lib/python3.6/site-packages/cntk/cntk_py.py\u001b[0m in \u001b[0;36mget_next_minibatch\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2917\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinibatchSource_get_next_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2918\u001b[0m \u001b[0mMinibatchSource_swigregister\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cntk_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMinibatchSource_swigregister\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m \u001b[0mMinibatchSource_swigregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMinibatchSource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function MinibatchSource_get_next_minibatch> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "path_log = \"log.txt\"\n",
    "file = open(path_log,'w')\n",
    "process1(5, 2, [128,512,1024], path_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "# from cntk.ops.functions import load_model\n",
    "# current_model = load_model(\"output/model/model4.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform testing data for prediction\n",
    "\n",
    "# def number_of_line(path):\n",
    "#     file = open(path, 'r')\n",
    "#     f = []\n",
    "#     while True :\n",
    "#         line = file.readline();\n",
    "#         if not line :\n",
    "#             break\n",
    "#         f.append(line)\n",
    "#     return len(f)\n",
    "\n",
    "# def prediction_file(path_directory):\n",
    "#     arr = []\n",
    "#     for kk in range(1,11) :\n",
    "#         if not kk == 3:\n",
    "#             path_file = \"traning_data_features_t\" + str(kk)+ \".txt\"\n",
    "#             size_file = number_of_line(path_directory + path_file)\n",
    "\n",
    "#             testing_file = os.path.join(path_directory, path_file)\n",
    "#             reader_test = create_reader(testing_file, False, input_dim, num_output_classes)\n",
    "#             test_input_map = {\n",
    "#                     input  : reader_test.streams.features,\n",
    "#                     label : reader_test.streams.labels\n",
    "#                 }\n",
    "#             data = reader_test.next_minibatch(size_file, input_map=test_input_map)\n",
    "#             data_asarray =  data[input].asarray()\n",
    "\n",
    "          \n",
    "#             for i in range(0, size_file):\n",
    "#                 patch = np.reshape(data_asarray[i], (3,31,31))\n",
    "#                 print(current_model.eval(patch)[0])\n",
    "#                 arr.append(current_model.eval(patch)[0])\n",
    "#     print(len(arr))\n",
    "#     return arr\n",
    "\n",
    "# arr = prediction_file(\"output_label/t/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #prediction and write answer in file \n",
    "\n",
    "# predict_labels = []\n",
    "\n",
    "# #prediction method => index : position label max\n",
    "# def predict_label(index) :\n",
    "#     if index == 0 :\n",
    "#         return \"1 0 0\"\n",
    "#     elif index == 1 :\n",
    "#         return \"0 1 0\"\n",
    "#     else :\n",
    "#         return \"0 0 1\"\n",
    "\n",
    "# path_file_prediction = \"output_label/prediction_label.txt\"\n",
    "# file = open(path_file_prediction, 'w')\n",
    "# file = open(path_file_prediction, 'a')\n",
    "\n",
    "# for current_arr in arr :\n",
    "#     file.write(predict_label(np.argmax(current_arr))+\"\\n\")\n",
    "#     predict_labels.append(predict_label(np.argmax(current_arr)))\n",
    "# file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test set data\n",
    "# labels = []\n",
    "# def get_label_from_testing_data(path):    \n",
    "#     file_test = open(path)\n",
    "#     while True :\n",
    "#         line = file_test.readline()\n",
    "#         if not line :\n",
    "#             break\n",
    "#         lines = line.split(\" \")\n",
    "#         current_label = lines[1] + \" \" + lines[2] + \" \" + lines[3]\n",
    "#         labels.append(current_label)\n",
    "\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t1.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t2.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t4.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t5.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t6.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t7.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t8.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t9.txt\")\n",
    "# get_label_from_testing_data(\"output_label/t/traning_data_features_t10.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_labels = len(labels)\n",
    "# t_bounary, t_inner, t_outer = 0, 0, 0\n",
    "# n_bounary_inner, n_bounary_outer = 0, 0\n",
    "# n_inner_bounary, n_inner_outer = 0, 0\n",
    "# n_outer_bounary, n_outer_inner = 0, 0,\n",
    "# for i in range(0, size_labels) :\n",
    "#     if labels[i] == \"0 0 1\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_bounary += 1 \n",
    "#         elif predict_labels[i] == \"0 1 0\" :\n",
    "#             n_bounary_inner += 1\n",
    "#         elif predict_labels[i] == \"1 0 0\" :\n",
    "#             n_bounary_outer += 1\n",
    "#     elif labels[i] == \"0 1 0\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_inner += 1 \n",
    "#         elif predict_labels[i] == \"0 0 1\" :\n",
    "#             n_inner_bounary += 1\n",
    "#         elif predict_labels[i] == \"1 0 0\" :\n",
    "#             n_inner_outer += 1\n",
    "#     elif labels[i] == \"1 0 0\":\n",
    "#         if labels[i] == predict_labels[i]:\n",
    "#             t_outer += 1 \n",
    "#         elif predict_labels[i] == \"0 0 1\" :\n",
    "#             n_outer_bounary += 1\n",
    "#         elif predict_labels[i] == \"0 1 0\" :\n",
    "#             n_outer_inner += 1\n",
    "# print(str(t_bounary) + \" \" + str(n_bounary_inner) + \" \" + str(n_bounary_outer))\n",
    "# print(str(n_inner_bounary) + \" \" + str(t_inner) + \" \" + str(n_inner_outer))\n",
    "# print(str(n_outer_bounary) + \" \" + str( n_outer_inner) + \" \" + str(n_bounary_outer))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
